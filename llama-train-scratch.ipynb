{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a basic implimentainion of the Llama 3 without crying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5]), tensor([[5, 3, 9, 6, 9]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary length. Llama's real vocab size is 128256. Here let's just use an absurdly small number\n",
    "v = 10\n",
    "\n",
    "# Llama's maximum sequence length is 8192, but for inference they cache 3/4 of it and only use an effective length of 2048. more on that later\n",
    "seq_len = 5\n",
    "\n",
    "# we'll use a batch size of 1 for simplicity when visualizing our tensors\n",
    "b = 1\n",
    "\n",
    "# now let's make ourselves a list of token indices. Each represents somewhere between a letter and a word\n",
    "tokens = torch.randint(v, (b, seq_len))\n",
    "tokens.shape, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initilizing the first residual state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 16]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.6211, -0.3046,  1.2291,  0.0654,  0.0206,  0.3026,  0.5760, -0.9168,\n",
       "          -1.3130, -0.6738, -2.5882, -2.4276, -1.4069,  1.1768, -0.9213, -0.4535],\n",
       "         [ 2.4885,  0.1856, -0.0444, -0.9305, -0.8918,  1.7270, -0.7148,  0.3457,\n",
       "           1.2337,  1.3811,  1.0488,  0.0633,  0.1826,  1.4127, -1.6101,  0.1325],\n",
       "         [-0.8538, -0.9467,  1.0717, -0.8660, -0.0293,  0.2173, -2.4219,  0.1731,\n",
       "           0.9239, -0.1767,  0.4690,  1.0902, -1.0578, -0.8805,  0.9535,  0.2885],\n",
       "         [ 1.0089, -0.0534, -0.7067,  0.1578, -0.3137, -0.5377,  0.1424, -0.1772,\n",
       "          -1.6768,  0.7786,  0.5583,  0.3120,  1.4882,  1.0504, -0.8779,  0.6144],\n",
       "         [-0.3958, -0.3728,  2.0452,  0.4676,  2.3109, -0.7293,  2.8377, -1.5640,\n",
       "           0.6458, -0.0217,  1.0460, -1.1739, -0.2615, -0.1555, -0.4311, -0.5748],\n",
       "         [-0.0508, -0.6860,  0.2296,  1.1494,  0.7252,  2.0619,  0.1963, -0.1768,\n",
       "          -0.3589, -1.4255, -0.0347,  1.7100, -3.1712,  0.1504, -0.8984,  0.5605],\n",
       "         [-0.2742, -0.5251, -1.4861, -2.5345,  0.6212, -1.4692, -0.5230, -0.4949,\n",
       "          -0.3542,  0.3015, -1.0694, -1.5897, -0.7937, -1.3195,  1.8830,  0.4581],\n",
       "         [-0.2638,  0.7374, -0.6766,  2.6759, -0.5838, -1.8064,  0.1004, -1.2577,\n",
       "           0.4002,  0.2569,  0.0190, -0.8082, -0.6290,  0.0075, -0.9694,  1.3008],\n",
       "         [ 0.4699,  1.1197, -0.1734,  0.1829,  1.2762, -0.2291,  0.2485, -0.7853,\n",
       "          -1.0042, -0.9042,  0.1217, -0.8024,  1.5925, -1.3602, -0.1563,  0.3123],\n",
       "         [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,  0.3237,\n",
       "           0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064, -0.1078,  0.6862]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding dimention for toy llama 3b but Llama 3 8b uses 4096\n",
    "d = 16\n",
    "\n",
    "# initilizing token embeddings matrix\n",
    "embedding = nn.Embedding(v, d)\n",
    "embedding.weight.shape, embedding.weight\n",
    "# each row in this embedding is high dimentinal representation of its corresponding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 16]),\n",
       " tensor([[[-0.0508, -0.6860,  0.2296,  1.1494,  0.7252,  2.0619,  0.1963,\n",
       "           -0.1768, -0.3589, -1.4255, -0.0347,  1.7100, -3.1712,  0.1504,\n",
       "           -0.8984,  0.5605],\n",
       "          [ 1.0089, -0.0534, -0.7067,  0.1578, -0.3137, -0.5377,  0.1424,\n",
       "           -0.1772, -1.6768,  0.7786,  0.5583,  0.3120,  1.4882,  1.0504,\n",
       "           -0.8779,  0.6144],\n",
       "          [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
       "            0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
       "           -0.1078,  0.6862],\n",
       "          [-0.2742, -0.5251, -1.4861, -2.5345,  0.6212, -1.4692, -0.5230,\n",
       "           -0.4949, -0.3542,  0.3015, -1.0694, -1.5897, -0.7937, -1.3195,\n",
       "            1.8830,  0.4581],\n",
       "          [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
       "            0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
       "           -0.1078,  0.6862]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grambbing the embeddings that correspond to our sequence of token indices\n",
    "x = embedding(tokens)\n",
    "x.shape, x\n",
    "# at this points many models would multiply the embeddings by the square root of the embedding dimension, but Llama 3 foregoes that strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute our RoPE Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs: torch.Size([2])\n",
      "tensor([1.0000, 0.0100])\n",
      "\n",
      "t: torch.Size([10])\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "\n",
      "freqs: torch.Size([10, 2])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [1.0000, 0.0100],\n",
      "        [2.0000, 0.0200],\n",
      "        [3.0000, 0.0300],\n",
      "        [4.0000, 0.0400],\n",
      "        [5.0000, 0.0500],\n",
      "        [6.0000, 0.0600],\n",
      "        [7.0000, 0.0700],\n",
      "        [8.0000, 0.0800],\n",
      "        [9.0000, 0.0900]])\n",
      "\n",
      "freqs_cis: torch.Size([5, 2])\n",
      "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
      "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
      "        [-0.4161+0.9093j,  0.9998+0.0200j],\n",
      "        [-0.9900+0.1411j,  0.9996+0.0300j],\n",
      "        [-0.6536-0.7568j,  0.9992+0.0400j]])\n"
     ]
    }
   ],
   "source": [
    "theta = 10000 # 10,000 is the most common value but Llama 3 uses 50,000. In theory smaller models should use a smaller value\n",
    "num_heads = 4 # Llama 3 8b has 32 total attention heads\n",
    "head_dim = d // num_heads # Llama 3 ties its head dimension to the embedding dimension. This value comes out to 128 in Llama 3, which is purposeful to\n",
    "\n",
    "freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
    "\n",
    "t = torch.arange(seq_len * 2, device=freqs.device, dtype=torch.float32)\n",
    "print(f't: {t.shape}\\n{t}\\n')\n",
    "\n",
    "freqs = torch.outer(t, freqs)\n",
    "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
    "\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)[:seq_len]  # complex64\n",
    "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Precomputing the Causal Mask\n",
    "<a id='d'></a>\n",
    "\n",
    "Similar to RoPE embeddings, the causal mask is another part of the attention mechanism that we can create ahead of time to then be reused in every layer.\n",
    "\n",
    "The basic idea of a causal mask is that by default, attention mechanisms allow every single token to pay attention to every single other token. This is okay or even preferable for some model types, but Llama is auto-regressive, meaning it would be bad if a given token to be predicted was able to see itself and future tokens during training but not during inference. The negative infinity's in the upper-triangle prevent the model from attending to the corresponding token; how this works will be more clear later when we do the attention softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(\n",
    "    (seq_len, seq_len),\n",
    "    float(\"-inf\")\n",
    ")\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization (RMS Norm)\n",
    "<a id='e'></a>\n",
    "\n",
    "Root Mean Square Normalization has also been the norm for quite awhile. Like its predecessor LayerNorm, RMSNorm restricts the variability of the entries in each embedding vector such that the vector lies on a hypersphere with radius $\\sqrt{d}$. However unlike LayerNorm which centers that hypersphere with a mean of zero, RMSNorm does not mess with the mean, which is an important source of data for networks that utilize residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: torch.Size([1, 5, 16])\n",
      "tensor([[[-0.0508, -0.6860,  0.2296,  1.1494,  0.7252,  2.0619,  0.1963,\n",
      "          -0.1768, -0.3589, -1.4255, -0.0347,  1.7100, -3.1712,  0.1504,\n",
      "          -0.8984,  0.5605],\n",
      "         [ 1.0089, -0.0534, -0.7067,  0.1578, -0.3137, -0.5377,  0.1424,\n",
      "          -0.1772, -1.6768,  0.7786,  0.5583,  0.3120,  1.4882,  1.0504,\n",
      "          -0.8779,  0.6144],\n",
      "         [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
      "           0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
      "          -0.1078,  0.6862],\n",
      "         [-0.2742, -0.5251, -1.4861, -2.5345,  0.6212, -1.4692, -0.5230,\n",
      "          -0.4949, -0.3542,  0.3015, -1.0694, -1.5897, -0.7937, -1.3195,\n",
      "           1.8830,  0.4581],\n",
      "         [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
      "           0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
      "          -0.1078,  0.6862]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first setup the residual connection that will be used later\n",
    "h = x\n",
    "print(f\"h: {h.shape}\\n{h}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4363],\n",
       "         [0.6424],\n",
       "         [0.8939],\n",
       "         [1.3787],\n",
       "         [0.8939]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perfroming first normalization\n",
    "# first squash each entry in x and then take the mean of those values across each embedding\n",
    "mean_squared = x.pow(2).mean(dim=-1,  keepdim=True)\n",
    "mean_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_normed: torch.Size([1, 5, 16])\n",
      "tensor([[[-0.0609, -0.8221,  0.2751,  1.3775,  0.8692,  2.4711,  0.2352,\n",
      "          -0.2119, -0.4301, -1.7084, -0.0416,  2.0494, -3.8005,  0.1803,\n",
      "          -1.0767,  0.6718],\n",
      "         [ 0.8086, -0.0428, -0.5664,  0.1265, -0.2514, -0.4310,  0.1142,\n",
      "          -0.1420, -1.3439,  0.6241,  0.4475,  0.2501,  1.1927,  0.8419,\n",
      "          -0.7036,  0.4925],\n",
      "         [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
      "           0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
      "          -0.1019,  0.6488],\n",
      "         [-0.3220, -0.6166, -1.7449, -2.9760,  0.7294, -1.7251, -0.6141,\n",
      "          -0.5811, -0.4159,  0.3540, -1.2557, -1.8666, -0.9319, -1.5493,\n",
      "           2.2110,  0.5379],\n",
      "         [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
      "           0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
      "          -0.1019,  0.6488]]], grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# then multiply x by the recirocal of the square roots of mean_squared\n",
    "# 1e-6 is very small number added for stability just in case an entry happens to be equal to 0 (since you can't divide by 0)\n",
    "x_normed = x / torch.rsqrt(mean_squared + 1e-6)\n",
    "print(f'x_normed: {x_normed.shape}\\n{x_normed}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms_scale: torch.Size([16])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "x_normed: torch.Size([1, 5, 16])\n",
      "tensor([[[-0.0609, -0.8221,  0.2751,  1.3775,  0.8692,  2.4711,  0.2352,\n",
      "          -0.2119, -0.4301, -1.7084, -0.0416,  2.0494, -3.8005,  0.1803,\n",
      "          -1.0767,  0.6718],\n",
      "         [ 0.8086, -0.0428, -0.5664,  0.1265, -0.2514, -0.4310,  0.1142,\n",
      "          -0.1420, -1.3439,  0.6241,  0.4475,  0.2501,  1.1927,  0.8419,\n",
      "          -0.7036,  0.4925],\n",
      "         [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
      "           0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
      "          -0.1019,  0.6488],\n",
      "         [-0.3220, -0.6166, -1.7449, -2.9760,  0.7294, -1.7251, -0.6141,\n",
      "          -0.5811, -0.4159,  0.3540, -1.2557, -1.8666, -0.9319, -1.5493,\n",
      "           2.2110,  0.5379],\n",
      "         [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
      "           0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
      "          -0.1019,  0.6488]]], grad_fn=<MulBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now time to multiply the learnable parameter the gamma and beta \n",
    "# This scale is initialized to 1's but if we were to train then these values will change\n",
    "rms_scale = torch.ones(d)\n",
    "print(f'rms_scale: {rms_scale.shape}\\n{rms_scale}\\n')\n",
    "\n",
    "x_normed *= rms_scale\n",
    "print(f'x_normed: {x_normed.shape}\\n{x_normed}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS function\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilize Multi-Query Attention\n",
    "<a id='f'></a>\n",
    "[multi-query attention](https://arxiv.org/abs/1911.02150) is the de facto standard for saving on parameter counts in order to get a bigger model. The idea is that the model can make multiple queries to the residual state and have those many queries be answered by shared keys & values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0508, -0.6860,  0.2296,  1.1494,  0.7252,  2.0619,  0.1963,\n",
       "           -0.1768, -0.3589, -1.4255, -0.0347,  1.7100, -3.1712,  0.1504,\n",
       "           -0.8984,  0.5605],\n",
       "          [ 1.0089, -0.0534, -0.7067,  0.1578, -0.3137, -0.5377,  0.1424,\n",
       "           -0.1772, -1.6768,  0.7786,  0.5583,  0.3120,  1.4882,  1.0504,\n",
       "           -0.8779,  0.6144],\n",
       "          [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
       "            0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
       "           -0.1078,  0.6862],\n",
       "          [-0.2742, -0.5251, -1.4861, -2.5345,  0.6212, -1.4692, -0.5230,\n",
       "           -0.4949, -0.3542,  0.3015, -1.0694, -1.5897, -0.7937, -1.3195,\n",
       "            1.8830,  0.4581],\n",
       "          [ 0.5823, -1.7120, -1.5994, -0.5196,  0.5663,  0.2875,  0.3768,\n",
       "            0.3237,  0.5569, -1.7216, -1.8272,  0.2705, -0.3602,  0.5064,\n",
       "           -0.1078,  0.6862]]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[[-0.0609, -0.8221,  0.2751,  1.3775,  0.8692,  2.4711,  0.2352,\n",
       "           -0.2119, -0.4301, -1.7084, -0.0416,  2.0494, -3.8005,  0.1803,\n",
       "           -1.0767,  0.6718],\n",
       "          [ 0.8086, -0.0428, -0.5664,  0.1265, -0.2514, -0.4310,  0.1142,\n",
       "           -0.1420, -1.3439,  0.6241,  0.4475,  0.2501,  1.1927,  0.8419,\n",
       "           -0.7036,  0.4925],\n",
       "          [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
       "            0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
       "           -0.1019,  0.6488],\n",
       "          [-0.3220, -0.6166, -1.7449, -2.9760,  0.7294, -1.7251, -0.6141,\n",
       "           -0.5811, -0.4159,  0.3540, -1.2557, -1.8666, -0.9319, -1.5493,\n",
       "            2.2110,  0.5379],\n",
       "          [ 0.5505, -1.6187, -1.5122, -0.4913,  0.5354,  0.2718,  0.3563,\n",
       "            0.3061,  0.5265, -1.6278, -1.7276,  0.2557, -0.3405,  0.4788,\n",
       "           -0.1019,  0.6488]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is for the residual connection and x_normed will go into our Attention calculation\n",
    "h, x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as a reminder: num_heads = 4, head_dim = 4\n"
     ]
    }
   ],
   "source": [
    "# let's define the hyperparameters of MQA\n",
    "num_kv_heads = 2 # Llama uses 8 key and value heads per layer\n",
    "assert num_heads % num_kv_heads == 0 # each q needs to match up to a kv\n",
    "print(f\"as a reminder: num_heads = {num_heads}, head_dim = {head_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  torch.Size([16, 16]) torch.Size([8, 16]) torch.Size([8, 16])\n",
      "Attention projections:  torch.Size([1, 5, 16]) torch.Size([1, 5, 8]) torch.Size([1, 5, 8])\n",
      "Reshaped:  torch.Size([1, 5, 4, 4]) torch.Size([1, 5, 2, 4]) torch.Size([1, 5, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# self-attention weight matrices\n",
    "wq = nn.Linear(d, num_heads * head_dim, bias=False)\n",
    "wk = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
    "wv = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
    "print(\"Attention weights: \", wq.weight.shape, wk.weight.shape, wv.weight.shape)\n",
    "\n",
    "# and project x_normed out to get our queries, keys and values\n",
    "xq = wq(x_normed)\n",
    "xk = wk(x_normed)\n",
    "xv = wv(x_normed)\n",
    "print(\"Attention projections: \", xq.shape, xk.shape, xv.shape)\n",
    "\n",
    "# then reshape them to separate out by head\n",
    "xq = xq.view(b, seq_len, num_heads, head_dim)\n",
    "xk = xk.view(b, seq_len, num_kv_heads, head_dim)\n",
    "xv = xv.view(b, seq_len, num_kv_heads, head_dim)\n",
    "print(\"Reshaped: \", xq.shape, xk.shape, xv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq: torch.Size([1, 5, 4, 2])\n",
      "tensor([[[[ 0.0605+0.4496j, -0.1676+0.9707j],\n",
      "          [-0.8873-0.5318j,  0.1048-0.2221j],\n",
      "          [ 0.8685-1.4743j, -1.2008+0.8642j],\n",
      "          [ 1.2625+0.1612j,  1.0077+0.7350j]],\n",
      "\n",
      "         [[-0.3397-0.1445j,  0.3150+0.8894j],\n",
      "          [-0.6021-0.1736j, -0.3982+0.0303j],\n",
      "          [-0.3812+0.3620j, -0.2967+0.2477j],\n",
      "          [-0.0091+0.2755j, -0.0595-0.0617j]],\n",
      "\n",
      "         [[-0.0409+0.1300j,  0.2640-0.4343j],\n",
      "          [-0.2360+0.1575j,  0.1715+0.1449j],\n",
      "          [ 0.7209-0.6044j, -0.5415+0.8812j],\n",
      "          [ 0.3178-1.5954j,  0.2347+0.1981j]],\n",
      "\n",
      "         [[ 0.7335+0.4946j,  0.1636-1.6794j],\n",
      "          [ 1.0972-0.1658j,  1.0036+0.3853j],\n",
      "          [ 0.7999+0.4196j,  0.2458-0.8357j],\n",
      "          [-0.7795-1.3275j, -1.5838+0.9322j]],\n",
      "\n",
      "         [[-0.0409+0.1300j,  0.2640-0.4343j],\n",
      "          [-0.2360+0.1575j,  0.1715+0.1449j],\n",
      "          [ 0.7209-0.6044j, -0.5415+0.8812j],\n",
      "          [ 0.3178-1.5954j,  0.2347+0.1981j]]]],\n",
      "       grad_fn=<ViewAsComplexBackward0>)\n",
      "\n",
      "xk: torch.Size([1, 5, 2, 2])\n",
      "tensor([[[[ 0.8117+1.9985e-01j, -0.4228+4.3245e-01j],\n",
      "          [-0.3582+8.5515e-01j, -0.5746+7.4674e-01j]],\n",
      "\n",
      "         [[ 0.7173-4.0113e-01j, -0.1348+1.0310e-01j],\n",
      "          [-0.2803-4.4054e-01j, -0.4614+4.5823e-04j]],\n",
      "\n",
      "         [[ 0.5364-7.1350e-01j,  0.0210+8.6937e-01j],\n",
      "          [-0.0344-1.0825e+00j, -0.6035+6.4116e-01j]],\n",
      "\n",
      "         [[-0.3734+2.0801e-01j, -1.0267-1.4650e-02j],\n",
      "          [ 0.4114-5.3187e-01j,  0.0867+1.6028e+00j]],\n",
      "\n",
      "         [[ 0.5364-7.1350e-01j,  0.0210+8.6937e-01j],\n",
      "          [-0.0344-1.0825e+00j, -0.6035+6.4116e-01j]]]],\n",
      "       grad_fn=<ViewAsComplexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xq = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "xk = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
    "print(f'xk: {xk.shape}\\n{xk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: [1, 5, 1, 2]\n",
      "\n",
      "freqs_cis: torch.Size([1, 5, 1, 2])\n",
      "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j]],\n",
      "\n",
      "         [[ 0.5403+0.8415j,  0.9999+0.0100j]],\n",
      "\n",
      "         [[-0.4161+0.9093j,  0.9998+0.0200j]],\n",
      "\n",
      "         [[-0.9900+0.1411j,  0.9996+0.0300j]],\n",
      "\n",
      "         [[-0.6536-0.7568j,  0.9992+0.0400j]]]])\n"
     ]
    }
   ],
   "source": [
    "ndim = xq.ndim\n",
    "assert 0 <= 1 < ndim\n",
    "assert freqs_cis.shape == (xq.shape[1], xq.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != xq.shape[1], xq.shape[-1] {(xq.shape[1], xq.shape[-1])}'\n",
    "\n",
    "# reshape our queries\n",
    "shape = [d if i == 1 or i == xq.ndim - 1 else 1 for i, d in enumerate(xq.shape)]\n",
    "print(f'shape: {shape}\\n')\n",
    "\n",
    "freqs_cis = freqs_cis.view(*shape)\n",
    "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq: torch.Size([1, 5, 4, 4])\n",
      "tensor([[[[ 0.0605,  0.4496, -0.1676,  0.9707],\n",
      "          [-0.8873, -0.5318,  0.1048, -0.2221],\n",
      "          [ 0.8685, -1.4743, -1.2008,  0.8642],\n",
      "          [ 1.2625,  0.1612,  1.0077,  0.7350]],\n",
      "\n",
      "         [[-0.0619, -0.3639,  0.3061,  0.8925],\n",
      "          [-0.1792, -0.6004, -0.3985,  0.0263],\n",
      "          [-0.5106, -0.1252, -0.2991,  0.2447],\n",
      "          [-0.2367,  0.1412, -0.0589, -0.0623]],\n",
      "\n",
      "         [[-0.1012, -0.0913,  0.2726, -0.4289],\n",
      "          [-0.0450, -0.2801,  0.1686,  0.1483],\n",
      "          [ 0.2495,  0.9070, -0.5590,  0.8702],\n",
      "          [ 1.3185,  0.9529,  0.2307,  0.2028]],\n",
      "\n",
      "         [[-0.7960, -0.3862,  0.2139, -1.6738],\n",
      "          [-1.0629,  0.3190,  0.9916,  0.4152],\n",
      "          [-0.8511, -0.3025,  0.2708, -0.8279],\n",
      "          [ 0.9590,  1.2042, -1.6110,  0.8843]],\n",
      "\n",
      "         [[ 0.1251, -0.0541,  0.2812, -0.4234],\n",
      "          [ 0.2734,  0.0757,  0.1656,  0.1517],\n",
      "          [-0.9286, -0.1506, -0.5763,  0.8588],\n",
      "          [-1.4151,  0.8024,  0.2266,  0.2073]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "xk: torch.Size([1, 5, 2, 4])\n",
      "tensor([[[[ 0.8117,  0.1999, -0.4228,  0.4324],\n",
      "          [-0.3582,  0.8551, -0.5746,  0.7467]],\n",
      "\n",
      "         [[ 0.7251,  0.3869, -0.1358,  0.1017],\n",
      "          [ 0.2192, -0.4739, -0.4613, -0.0042]],\n",
      "\n",
      "         [[ 0.4256,  0.7847,  0.0036,  0.8696],\n",
      "          [ 0.9986,  0.4192, -0.6162,  0.6290]],\n",
      "\n",
      "         [[ 0.3403, -0.2586, -1.0258, -0.0454],\n",
      "          [-0.3322,  0.5846,  0.0386,  1.6047]],\n",
      "\n",
      "         [[-0.8906,  0.0604, -0.0138,  0.8695],\n",
      "          [-0.7967,  0.7336, -0.6287,  0.6165]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now multiply the data by the frequencies, turn them back into real numbers, revert the shape and make sure they're of the right type\n",
    "xq = torch.view_as_real(xq * freqs_cis).flatten(3).type_as(xv)\n",
    "xk = torch.view_as_real(xk * freqs_cis).flatten(3).type_as(xv)\n",
    "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
    "print(f'xk: {xk.shape}\\n{xk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the number of K & V heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "if num_kv_heads != num_heads:\n",
    "  num_queries_per_kv = num_heads // num_kv_heads\n",
    "  xk = torch.repeat_interleave(xk, num_queries_per_kv, dim=2)\n",
    "  xv = torch.repeat_interleave(xv, num_queries_per_kv, dim=2)\n",
    "\n",
    "xq.shape, xk.shape, xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "xq = xq.transpose(1, 2)\n",
    "xk = xk.transpose(1, 2)\n",
    "xv = xv.transpose(1, 2)\n",
    "\n",
    "xq.shape, xk.shape, xv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5, 5]),\n",
       " tensor([[[[ 0.3148,  0.1697,  0.6111,  0.0161,  0.4098],\n",
       "           [ 0.0668, -0.0682,  0.2327, -0.1408,  0.4025],\n",
       "           [-0.2006, -0.0947, -0.2434, -0.1355, -0.1460],\n",
       "           [-0.7688, -0.4630, -1.0483, -0.1572, -0.3864],\n",
       "           [-0.1056, -0.0057, -0.1782, -0.1063, -0.2434]],\n",
       " \n",
       "          [[-0.4835, -0.4430, -0.4939, -0.1309,  0.2818],\n",
       "           [-0.0428, -0.1527, -0.2630,  0.2509,  0.0759],\n",
       "           [-0.0498, -0.0744, -0.0547, -0.0613,  0.0749],\n",
       "           [-0.5194, -0.3698,  0.0813, -0.7402,  0.6566],\n",
       "           [ 0.1163,  0.1102,  0.1541, -0.0516, -0.0547]],\n",
       " \n",
       "          [[-0.1182,  0.7197,  0.7664,  0.0950, -0.2429],\n",
       "           [ 0.2152,  0.0422, -0.1120,  0.2388,  0.3269],\n",
       "           [ 0.8286, -0.0604,  0.7606,  0.9111,  0.6772],\n",
       "           [-0.3638, -0.0824, -0.8322, -0.6061, -0.1122],\n",
       "           [ 0.5882,  0.0650, -0.0476,  0.7882,  0.7606]],\n",
       " \n",
       "          [[-0.1723, -0.1338,  0.5848,  0.4465, -0.5340],\n",
       "           [ 0.0964, -0.0457, -0.0901,  0.0295,  0.1454],\n",
       "           [ 0.1807, -0.1349,  0.8507,  0.2266, -0.1857],\n",
       "           [ 1.1361,  0.1896,  1.5057,  0.8711,  0.8386],\n",
       "           [ 0.6088, -0.3980, -0.5430,  0.6404,  0.8507]]]],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
    "scores = torch.matmul(xq, xk.transpose(2, 3))\n",
    "\n",
    "# then we scale the logits by the reciprocal of the square root of the head dimension\n",
    "scores = scores / math.sqrt(head_dim)\n",
    "\n",
    "scores.shape, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5, 5]),\n",
       " tensor([[[[ 0.3148,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.0668, -0.0682,    -inf,    -inf,    -inf],\n",
       "           [-0.2006, -0.0947, -0.2434,    -inf,    -inf],\n",
       "           [-0.7688, -0.4630, -1.0483, -0.1572,    -inf],\n",
       "           [-0.1056, -0.0057, -0.1782, -0.1063, -0.2434]],\n",
       " \n",
       "          [[-0.4835,    -inf,    -inf,    -inf,    -inf],\n",
       "           [-0.0428, -0.1527,    -inf,    -inf,    -inf],\n",
       "           [-0.0498, -0.0744, -0.0547,    -inf,    -inf],\n",
       "           [-0.5194, -0.3698,  0.0813, -0.7402,    -inf],\n",
       "           [ 0.1163,  0.1102,  0.1541, -0.0516, -0.0547]],\n",
       " \n",
       "          [[-0.1182,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.2152,  0.0422,    -inf,    -inf,    -inf],\n",
       "           [ 0.8286, -0.0604,  0.7606,    -inf,    -inf],\n",
       "           [-0.3638, -0.0824, -0.8322, -0.6061,    -inf],\n",
       "           [ 0.5882,  0.0650, -0.0476,  0.7882,  0.7606]],\n",
       " \n",
       "          [[-0.1723,    -inf,    -inf,    -inf,    -inf],\n",
       "           [ 0.0964, -0.0457,    -inf,    -inf,    -inf],\n",
       "           [ 0.1807, -0.1349,  0.8507,    -inf,    -inf],\n",
       "           [ 1.1361,  0.1896,  1.5057,  0.8711,    -inf],\n",
       "           [ 0.6088, -0.3980, -0.5430,  0.6404,  0.8507]]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the mask that we precomputed earlier\n",
    "scores = scores + mask\n",
    "\n",
    "scores.shape, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5337, 0.4663, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3258, 0.3621, 0.3121, 0.0000, 0.0000],\n",
       "          [0.2017, 0.2739, 0.1525, 0.3718, 0.0000],\n",
       "          [0.2038, 0.2253, 0.1896, 0.2037, 0.1776]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5275, 0.4725, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3366, 0.3284, 0.3350, 0.0000, 0.0000],\n",
       "          [0.2089, 0.2426, 0.3809, 0.1675, 0.0000],\n",
       "          [0.2118, 0.2106, 0.2200, 0.1791, 0.1785]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5431, 0.4569, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4264, 0.1753, 0.3983, 0.0000, 0.0000],\n",
       "          [0.2677, 0.3547, 0.1676, 0.2101, 0.0000],\n",
       "          [0.2207, 0.1308, 0.1169, 0.2695, 0.2622]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5355, 0.4645, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2715, 0.1980, 0.5305, 0.0000, 0.0000],\n",
       "          [0.2776, 0.1077, 0.4017, 0.2130, 0.0000],\n",
       "          [0.2508, 0.0916, 0.0793, 0.2588, 0.3194]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we perform the softmax operation to get our actual probabilities\n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "scores\n",
    "# notice that thanks to the causal mask, 0 probability is placed on future tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 5, 4]),\n",
       " tensor([[[[-0.8451,  0.1792, -0.7642, -1.6837],\n",
       "           [-0.6167, -0.1819, -0.2313, -0.8675],\n",
       "           [-0.3947, -0.1515, -0.1678, -0.8775],\n",
       "           [-0.0391, -0.2616,  0.2056, -0.3395],\n",
       "           [-0.1186, -0.1662,  0.0189, -0.6591]],\n",
       " \n",
       "          [[-0.8451,  0.1792, -0.7642, -1.6837],\n",
       "           [-0.6137, -0.1867, -0.2242, -0.8566],\n",
       "           [-0.3912, -0.1291, -0.1930, -0.9239],\n",
       "           [-0.1505, -0.1620, -0.0085, -0.6970],\n",
       "           [-0.1340, -0.1464, -0.0172, -0.7192]],\n",
       " \n",
       "          [[ 0.1289,  0.1438,  0.1470, -2.1349],\n",
       "           [-0.1190,  0.1893,  0.0623, -1.2832],\n",
       "           [-0.0828,  0.0757,  0.1174, -1.2552],\n",
       "           [ 0.0698,  0.0708, -0.1399, -0.6530],\n",
       "           [ 0.1811, -0.0174, -0.1598, -0.6104]],\n",
       " \n",
       "          [[ 0.1289,  0.1438,  0.1470, -2.1349],\n",
       "           [-0.1231,  0.1901,  0.0609, -1.2689],\n",
       "           [-0.1338,  0.0496,  0.1141, -1.0293],\n",
       "           [ 0.1378, -0.0051, -0.0955, -0.7803],\n",
       "           [ 0.1873, -0.0219, -0.1411, -0.6860]]]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then matmul by our values projection\n",
    "output = torch.matmul(scores, xv)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 16]),\n",
       " tensor([[[-0.8451,  0.1792, -0.7642, -1.6837, -0.8451,  0.1792, -0.7642,\n",
       "           -1.6837,  0.1289,  0.1438,  0.1470, -2.1349,  0.1289,  0.1438,\n",
       "            0.1470, -2.1349],\n",
       "          [-0.6167, -0.1819, -0.2313, -0.8675, -0.6137, -0.1867, -0.2242,\n",
       "           -0.8566, -0.1190,  0.1893,  0.0623, -1.2832, -0.1231,  0.1901,\n",
       "            0.0609, -1.2689],\n",
       "          [-0.3947, -0.1515, -0.1678, -0.8775, -0.3912, -0.1291, -0.1930,\n",
       "           -0.9239, -0.0828,  0.0757,  0.1174, -1.2552, -0.1338,  0.0496,\n",
       "            0.1141, -1.0293],\n",
       "          [-0.0391, -0.2616,  0.2056, -0.3395, -0.1505, -0.1620, -0.0085,\n",
       "           -0.6970,  0.0698,  0.0708, -0.1399, -0.6530,  0.1378, -0.0051,\n",
       "           -0.0955, -0.7803],\n",
       "          [-0.1186, -0.1662,  0.0189, -0.6591, -0.1340, -0.1464, -0.0172,\n",
       "           -0.7192,  0.1811, -0.0174, -0.1598, -0.6104,  0.1873, -0.0219,\n",
       "           -0.1411, -0.6860]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and reshape to put the sequence length back into place and the outputs of our heads lined up\n",
    "output = output.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 16]),\n",
       " tensor([[[ 4.7982e-01,  1.2011e+00, -3.2596e-01, -3.0273e-01,  1.2942e+00,\n",
       "           -1.5138e+00,  4.5424e-01, -3.9018e-01,  1.1150e-01, -5.3451e-01,\n",
       "            9.0415e-01, -1.0406e-01,  4.7493e-01,  9.8201e-02, -1.9394e-01,\n",
       "           -7.4698e-01],\n",
       "          [ 2.0381e-01,  4.9071e-01, -8.6177e-02, -1.0249e-01,  8.0522e-01,\n",
       "           -8.4437e-01,  2.3502e-01, -9.6294e-02,  2.2250e-01, -2.2936e-01,\n",
       "            4.1498e-01, -3.8364e-02,  1.6870e-01,  5.7585e-02, -1.7539e-01,\n",
       "           -3.1306e-01],\n",
       "          [ 2.8165e-01,  4.9939e-01, -1.2491e-01, -9.5168e-02,  6.8374e-01,\n",
       "           -7.3566e-01,  1.7579e-01, -5.3414e-02,  1.0993e-01, -1.8658e-01,\n",
       "            3.0999e-01,  8.2164e-04,  2.6105e-01,  1.4615e-02, -1.1898e-01,\n",
       "           -2.1451e-01],\n",
       "          [ 1.6622e-01,  2.2077e-01, -1.2813e-01, -6.7325e-02,  3.9521e-01,\n",
       "           -4.2479e-01,  1.1963e-01,  5.0992e-02, -1.8103e-02,  5.8158e-02,\n",
       "            2.5292e-01,  3.8821e-02,  1.7923e-01,  8.8306e-02, -1.6750e-01,\n",
       "           -1.1223e-01],\n",
       "          [ 1.4791e-01,  2.7290e-01, -1.5961e-01, -1.3471e-01,  4.2885e-01,\n",
       "           -3.8738e-01,  1.4212e-01, -6.1740e-02, -3.2553e-02, -1.1177e-01,\n",
       "            1.7486e-01,  7.6697e-02,  1.8145e-01,  1.0898e-01, -1.7344e-01,\n",
       "           -9.2926e-02]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally initializing and apply output projection that mixes the information from the heads together\n",
    "wo = nn.Linear(num_heads * head_dim, d, bias=False)\n",
    "Xout = wo(output)\n",
    "Xout.shape, Xout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Residual Conenction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 16]),\n",
       " tensor([[[ 0.4290,  0.5151, -0.0964,  0.8467,  2.0194,  0.5481,  0.6505,\n",
       "           -0.5670, -0.2474, -1.9600,  0.8694,  1.6060, -2.6963,  0.2486,\n",
       "           -1.0924, -0.1864],\n",
       "          [ 1.2127,  0.4373, -0.7929,  0.0553,  0.4915, -1.3821,  0.3775,\n",
       "           -0.2735, -1.4543,  0.5493,  0.9733,  0.2736,  1.6569,  1.1080,\n",
       "           -1.0533,  0.3014],\n",
       "          [ 0.8639, -1.2126, -1.7243, -0.6148,  1.2500, -0.4482,  0.5526,\n",
       "            0.2703,  0.6668, -1.9082, -1.5172,  0.2713, -0.0991,  0.5210,\n",
       "           -0.2268,  0.4716],\n",
       "          [-0.1080, -0.3044, -1.6142, -2.6018,  1.0164, -1.8940, -0.4034,\n",
       "           -0.4439, -0.3723,  0.3597, -0.8165, -1.5508, -0.6144, -1.2312,\n",
       "            1.7155,  0.3459],\n",
       "          [ 0.7302, -1.4391, -1.7590, -0.6543,  0.9951, -0.0999,  0.5189,\n",
       "            0.2620,  0.5243, -1.8334, -1.6524,  0.3472, -0.1787,  0.6154,\n",
       "           -0.2812,  0.5932]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h += Xout\n",
    "h.shape, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the current state of our residual for use in our MoE later\n",
    "pre_ffwd_norm = RMSNorm(d)\n",
    "h_normed = pre_ffwd_norm(h)\n",
    "# so now we're working with x, which we'll use later for our next residual conenction, and x_normed which is used by our MoE MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SwiGLU Feedforward Network\n",
    "<a id='j'></a>\n",
    "\n",
    "Llama 3 models have surprisingly not opted for a mixture of experts strategy which i was assuming they'd go for by now. Their feedforward networks use the SwiGLU activation which basically uses the activation function as a gate that dynamically determines what information gets through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "42\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "# first we need to define our actual hidden dimension, which Llama's code does in an unnecessarily complicated manner\n",
    "hidden_dim = 4 * d # usually i would designate a hyperparameter for this 4, but in llama's code it was just there\n",
    "print(hidden_dim)\n",
    "hidden_dim = int(2 * hidden_dim / 3)\n",
    "print(hidden_dim)\n",
    "multiple_of = 256 # their description of this was \"make SwiGLU hidden layer size multiple of large power of 2\"\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "print(hidden_dim)\n",
    "# so basically this overly convoluted setup is designed to ensure that hidden_dim is a multiple of 256, likely for hardware efficiency reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = nn.Linear(d, hidden_dim, bias=False)\n",
    "gate = nn.Linear(d, hidden_dim, bias=False)\n",
    "down = nn.Linear(hidden_dim, d, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256]) tensor([[[-0.0642, -0.1545, -0.2192,  ...,  0.9438,  0.0570, -0.3783],\n",
      "         [ 0.4076, -0.3049,  0.1735,  ...,  0.2229,  0.2467, -0.5810],\n",
      "         [-0.0606,  0.7448,  0.9840,  ...,  0.6000,  0.2958, -1.6931],\n",
      "         [ 0.3540,  0.0034,  0.6070,  ..., -0.2506,  0.7748, -0.9145],\n",
      "         [-0.2998,  0.8691,  1.0339,  ...,  0.3945,  0.1622, -1.5953]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "up_proj = up(h_normed)\n",
    "print(up_proj.shape, up_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 256]) tensor([[[-0.0687, -0.1480, -0.1143,  ...,  0.0761, -0.1955,  0.3848],\n",
      "         [ 0.1410,  0.2848, -0.1783,  ...,  0.4109, -0.2723,  0.3605],\n",
      "         [ 0.1933,  0.6690,  0.1321,  ...,  0.3607, -0.1581,  0.1268],\n",
      "         [ 0.0360,  0.3590,  0.5313,  ...,  0.3714,  0.0489,  0.0404],\n",
      "         [ 0.1203,  0.6909,  0.1560,  ...,  0.3374, -0.1442,  0.0950]]],\n",
      "       grad_fn=<SiluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gate_proj = F.silu(gate(h_normed))\n",
    "print(gate_proj.shape, gate_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16]) tensor([[[ 0.0179, -0.0192, -0.1657, -0.0852,  0.0557, -0.0963, -0.1082,\n",
      "          -0.0335,  0.0246,  0.0104, -0.0752,  0.0661, -0.1234, -0.0924,\n",
      "           0.0812, -0.1590],\n",
      "         [-0.0547,  0.0771,  0.1036, -0.1620, -0.0560,  0.0905, -0.3136,\n",
      "           0.0094,  0.0883, -0.0112, -0.1757,  0.0637, -0.1961, -0.0808,\n",
      "           0.1623, -0.0319],\n",
      "         [-0.0080, -0.1601,  0.0289,  0.0709,  0.0767,  0.2686,  0.0853,\n",
      "           0.0287, -0.1481, -0.0976, -0.1968, -0.0298, -0.2074,  0.0245,\n",
      "           0.0166, -0.0764],\n",
      "         [-0.0239, -0.1397, -0.0004, -0.0705,  0.1095,  0.1232, -0.0401,\n",
      "           0.0346, -0.1058, -0.0731, -0.1919,  0.1236, -0.0659,  0.0992,\n",
      "           0.0396, -0.1036],\n",
      "         [ 0.0255, -0.1310,  0.0366,  0.0466,  0.0820,  0.2912,  0.0744,\n",
      "           0.0321, -0.0764, -0.1134, -0.1699, -0.0249, -0.2140,  0.0253,\n",
      "           0.0200, -0.1287]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ffwd_output = down(up_proj * gate_proj)\n",
    "print(ffwd_output.shape, ffwd_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16]) tensor([[[ 0.4469,  0.4960, -0.2621,  0.7615,  2.0751,  0.4517,  0.5423,\n",
      "          -0.6005, -0.2227, -1.9495,  0.7942,  1.6721, -2.8197,  0.1563,\n",
      "          -1.0112, -0.3454],\n",
      "         [ 1.1579,  0.5144, -0.6892, -0.1067,  0.4355, -1.2916,  0.0639,\n",
      "          -0.2641, -1.3660,  0.5381,  0.7976,  0.3373,  1.4607,  1.0272,\n",
      "          -0.8911,  0.2695],\n",
      "         [ 0.8559, -1.3728, -1.6954, -0.5439,  1.3268, -0.1796,  0.6378,\n",
      "           0.2991,  0.5187, -2.0058, -1.7140,  0.2414, -0.3065,  0.5455,\n",
      "          -0.2102,  0.3952],\n",
      "         [-0.1319, -0.4440, -1.6146, -2.6724,  1.1260, -1.7708, -0.4435,\n",
      "          -0.4093, -0.4781,  0.2866, -1.0083, -1.4273, -0.6804, -1.1320,\n",
      "           1.7551,  0.2423],\n",
      "         [ 0.7557, -1.5701, -1.7224, -0.6078,  1.0771,  0.1913,  0.5933,\n",
      "           0.2941,  0.4479, -1.9468, -1.8223,  0.3222, -0.3927,  0.6407,\n",
      "          -0.2613,  0.4646]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# and then do our final residual connection of this layer\n",
    "out = h + ffwd_output\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "<a id='k'></a>\n",
    "So usually we'd run it back on steps 1e through 1j for however many layers our model has (Llama 3 8b uses 32) using different weight matrices but you get the point. Since our current `out` is of the same shape that it would be if we were to do more layers, let's go ahead and just see what Llama's output mechanism looks like. It's nothing interesting though, just a linear layer. Notably they chose to use a separate linear layer rather than re-using the embedding layer as is relatively common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we norm the residual state\n",
    "final_norm = RMSNorm(d)\n",
    "out_normed = final_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 10]),\n",
       " tensor([[[-0.0716, -0.2248,  0.3506,  0.1224,  0.0167, -0.1844,  0.1912,\n",
       "            0.6053,  0.5315, -0.3186],\n",
       "          [ 0.5396, -1.4051,  0.7336, -0.1780,  0.2618, -0.5472,  0.2408,\n",
       "           -0.6137,  0.3824, -1.2147],\n",
       "          [-0.1733, -0.3616,  0.4563,  0.0155,  0.3227,  0.7008,  0.0223,\n",
       "            0.2701, -0.3240,  0.1441],\n",
       "          [-0.8815,  0.4323, -0.8879,  1.0441, -0.6592,  0.8555, -0.6688,\n",
       "            0.5801, -1.1062,  0.8051],\n",
       "          [-0.2352, -0.3454,  0.3589, -0.0648,  0.2186,  0.7198,  0.1076,\n",
       "            0.2584, -0.2721,  0.1699]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then multiply by the linear layer to get our final output logits\n",
    "final_output = nn.Linear(d, v, bias=False)\n",
    "logits = final_output(out_normed).float()\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0803, 0.0689, 0.1224, 0.0974, 0.0877, 0.0717, 0.1044, 0.1579,\n",
       "          0.1467, 0.0627],\n",
       "         [0.1660, 0.0237, 0.2015, 0.0810, 0.1257, 0.0560, 0.1231, 0.0524,\n",
       "          0.1418, 0.0287],\n",
       "         [0.0716, 0.0593, 0.1344, 0.0865, 0.1176, 0.1717, 0.0871, 0.1116,\n",
       "          0.0616, 0.0984],\n",
       "         [0.0320, 0.1190, 0.0318, 0.2195, 0.0400, 0.1818, 0.0396, 0.1380,\n",
       "          0.0256, 0.1728],\n",
       "         [0.0686, 0.0614, 0.1243, 0.0814, 0.1080, 0.1783, 0.0967, 0.1124,\n",
       "          0.0661, 0.1029]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax the logits to get the probability for each token's prediction across every token in the sequence\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 2, 5, 3, 5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedily decode the probabilities to get our final predicted indices\n",
    "greedy_indices = torch.argmax(probs, dim=-1)\n",
    "greedy_indices\n",
    "# if we were performing inference rather than training, that final token in the list would be the one to show the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss functions\n",
    "<a id='l'></a>\n",
    "\n",
    "Of course we use [cross-entropy loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) which should need no introduction if this isn't your first machine-learning rodeo, so we'll be skimming past it. Basically the idea is that the single correct value is rewarded and all other values are suppressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 6, 9, 4, 7]])\n",
      "tensor(1.9709, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create some random fake target indices to train on\n",
    "target_token_indices = torch.randint(0, v, greedy_indices.shape)\n",
    "print(target_token_indices)\n",
    "\n",
    "# initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# reshape logits to be compatible and calculate loss\n",
    "loss = loss_fn(logits.view(1,v,seq_len), target_token_indices)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's code everything up the correct way into classes so that we can actually build a functioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll be using a crazy small & simple tokenizer based on the TinyShakespeare dataset\n",
    "Llama 3 8b's vocabulary size is 128256 including special tokens like <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "\n",
    "def load_tokenizer_data(size: int):\n",
    "    file_name = f'./tokenizers/tiny_shakespeare_tokenizer_{size}.model'\n",
    "    with open(file_name, 'rb') as f:\n",
    "        tokenizer_data = pickle.load(f)\n",
    "    return tokenizer_data\n",
    "\n",
    "def get_tokenizer(size: int):\n",
    "    tokenizer_data = load_tokenizer_data(size)\n",
    "    loaded_stoi = tokenizer_data['stoi']\n",
    "    loaded_merges = tokenizer_data['merges']\n",
    "    return SimpleTokenizer(loaded_stoi, loaded_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tokenizers/tiny_shakespeare_tokenizer_512.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 57\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tokenizer\u001b[39m(size: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m---> 57\u001b[0m     tokenizer_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_tokenizer_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     loaded_stoi \u001b[38;5;241m=\u001b[39m tokenizer_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoi\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     59\u001b[0m     loaded_merges \u001b[38;5;241m=\u001b[39m tokenizer_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerges\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[41], line 52\u001b[0m, in \u001b[0;36mload_tokenizer_data\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tokenizer_data\u001b[39m(size: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m     51\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tokenizers/tiny_shakespeare_tokenizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     53\u001b[0m         tokenizer_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_data\n",
      "File \u001b[1;32mc:\\Users\\vigne\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tokenizers/tiny_shakespeare_tokenizer_512.model'"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
